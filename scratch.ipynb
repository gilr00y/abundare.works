{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import torch.onnx\n",
    "\n",
    "import fast_neural_style.neural_style.utils as utils\n",
    "from fast_neural_style.neural_style.transformer_net import TransformerNet\n",
    "from fast_neural_style.neural_style.vgg import Vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vg = models.vgg16(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU training unavailable... using CPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('CUDA available, using GPU.')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('GPU training unavailable... using CPU.')\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`python neural_style/neural_style.py train --dataset images/train-images --save-model-dir snapshots/ --cuda 1 --style-image images/style-images/scream_painting.jpg --epochs 40 --batch-size 10 --lr 6e-3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 19 00:35:21 2019\tEpoch 1:\t[4/124]\tcontent: 30.216785\tstyle: 0.139574\ttotal: 30.356359\n",
      "Mon Aug 19 00:35:34 2019\tEpoch 1:\t[8/124]\tcontent: 27.128148\tstyle: 0.143958\ttotal: 27.272106\n",
      "Mon Aug 19 00:35:46 2019\tEpoch 1:\t[12/124]\tcontent: 26.754836\tstyle: 0.143421\ttotal: 26.898257\n",
      "Mon Aug 19 00:35:58 2019\tEpoch 1:\t[16/124]\tcontent: 26.947499\tstyle: 0.143620\ttotal: 27.091119\n",
      "Mon Aug 19 00:36:11 2019\tEpoch 1:\t[20/124]\tcontent: 25.242469\tstyle: 0.144105\ttotal: 25.386574\n",
      "Mon Aug 19 00:36:23 2019\tEpoch 1:\t[24/124]\tcontent: 24.105388\tstyle: 0.145021\ttotal: 24.250409\n",
      "Mon Aug 19 00:36:35 2019\tEpoch 1:\t[28/124]\tcontent: 23.415253\tstyle: 0.145175\ttotal: 23.560429\n",
      "Mon Aug 19 00:36:47 2019\tEpoch 1:\t[32/124]\tcontent: 23.087648\tstyle: 0.145247\ttotal: 23.232895\n",
      "Mon Aug 19 00:37:00 2019\tEpoch 1:\t[36/124]\tcontent: 23.293119\tstyle: 0.145142\ttotal: 23.438261\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d1293e2cb52a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstyle_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "image_size = 256\n",
    "style_size = 256\n",
    "\n",
    "epochs = 3\n",
    "dataset = 'fast_neural_style/images/train-images/'\n",
    "batch_size = 4\n",
    "lr = 1e-3\n",
    "# If starting from existing model\n",
    "model = 'fast_neural_style/snapshots/epoch_1000_Fri_Jul_12_18:53:26_2019_100000.0_10000000000.0.model'\n",
    "\n",
    "checkpoint_model_dir = './'\n",
    "checkpoint_interval = 20\n",
    "\n",
    "content_weight = 3\n",
    "style_weight = 1000\n",
    "\n",
    "style_image = 'fast_neural_style/images/style-images/scream_painting.jpg'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.mul(255))\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    dataset, transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "# Image transformation network.\n",
    "transformer = TransformerNet()\n",
    "\n",
    "if model:\n",
    "    state_dict = torch.load(model)\n",
    "    transformer.load_state_dict(state_dict)\n",
    "\n",
    "transformer.to(device)\n",
    "\n",
    "optimizer = Adam(transformer.parameters(), lr=lr)\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "# Loss Network: VGG16\n",
    "vgg = Vgg16(requires_grad=False).to(device)\n",
    "style_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.mul(255))\n",
    "])\n",
    "\n",
    "style = utils.load_image(style_image, size=style_size)\n",
    "style = style_transform(style)\n",
    "style = style.repeat(batch_size, 1, 1, 1).to(device)\n",
    "\n",
    "features_style = vgg(utils.normalize_batch(style))\n",
    "gram_style = [utils.gram_matrix(y) for y in features_style]\n",
    "\n",
    "for e in range(epochs):\n",
    "    transformer.train()\n",
    "    agg_content_loss = 0.\n",
    "    agg_style_loss = 0.\n",
    "    count = 0\n",
    "    for batch_id, (x, _) in enumerate(train_loader):\n",
    "        n_batch = len(x)\n",
    "        count += n_batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # CUDA if available\n",
    "        x = x.to(device)\n",
    "\n",
    "        # Transform image\n",
    "        y = transformer(x)\n",
    "\n",
    "        y = utils.normalize_batch(y)\n",
    "        x = utils.normalize_batch(x)\n",
    "\n",
    "        # Feature Map of original image\n",
    "        features_x = vgg(x)\n",
    "        # Feature Map of transformed image\n",
    "        features_y = vgg(y)\n",
    "\n",
    "        # Difference between transformed image, original image.\n",
    "        content_loss = content_weight * mse_loss(features_y.relu3_3, features_x.relu3_3)\n",
    "\n",
    "        # Compute gram matrix \n",
    "        style_loss = 0.\n",
    "        for ft_y, gm_s in zip(features_y, gram_style):\n",
    "            gm_y = utils.gram_matrix(ft_y)\n",
    "            style_loss += mse_loss(gm_y, gm_s[:n_batch, :, :])\n",
    "        style_loss *= style_weight\n",
    "\n",
    "        total_loss = content_loss + style_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        agg_content_loss += content_loss.item()\n",
    "        agg_style_loss += style_loss.item()\n",
    "\n",
    "        if True: #(batch_id + 1) % args.log_interval == 0:\n",
    "            mesg = \"{}\\tEpoch {}:\\t[{}/{}]\\tcontent: {:.6f}\\tstyle: {:.6f}\\ttotal: {:.6f}\".format(\n",
    "                time.ctime(), e + 1, count, len(train_dataset),\n",
    "                              agg_content_loss / (batch_id + 1),\n",
    "                              agg_style_loss / (batch_id + 1),\n",
    "                              (agg_content_loss + agg_style_loss) / (batch_id + 1)\n",
    "            )\n",
    "            print(mesg)\n",
    "\n",
    "        if checkpoint_model_dir is not None and (batch_id + 1) % checkpoint_interval == 0:\n",
    "            transformer.eval().cpu()\n",
    "            ckpt_model_filename = \"ckpt_epoch_\" + str(e) + \"_batch_id_\" + str(batch_id + 1) + \".pth\"\n",
    "            ckpt_model_path = os.path.join(checkpoint_model_dir, ckpt_model_filename)\n",
    "            torch.save(transformer.state_dict(), ckpt_model_path)\n",
    "            transformer.to(device).train()\n",
    "\n",
    "# save model\n",
    "transformer.eval().cpu()\n",
    "save_model_filename = \"epoch_\" + str(epochs) + \"_\" + str(time.ctime()).replace(' ', '_') + \"_\" + str(\n",
    "    content_weight) + \"_\" + str(style_weight) + \".model\"\n",
    "save_model_path = os.path.join(save_model_dir, save_model_filename)\n",
    "torch.save(transformer.state_dict(), save_model_path)\n",
    "\n",
    "print(\"\\nDone, trained model saved at\", save_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-97b6730a6d93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Stylize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcontent_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m content_transform = transforms.Compose([\n\u001b[1;32m      4\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# Stylize\n",
    "content_image = utils.load_image(args.content_image, scale=args.content_scale)\n",
    "content_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.mul(255))\n",
    "])\n",
    "content_image = content_transform(content_image)\n",
    "content_image = content_image.unsqueeze(0).to(device)\n",
    "\n",
    "if args.model.endswith(\".onnx\"):\n",
    "    output = stylize_onnx_caffe2(content_image, args)\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        style_model = TransformerNet()\n",
    "        state_dict = torch.load(args.model)\n",
    "        # remove saved deprecated running_* keys in InstanceNorm from the checkpoint\n",
    "        for k in list(state_dict.keys()):\n",
    "            if re.search(r'in\\d+\\.running_(mean|var)$', k):\n",
    "                del state_dict[k]\n",
    "        style_model.load_state_dict(state_dict)\n",
    "        style_model.to(device)\n",
    "        if args.export_onnx:\n",
    "            assert args.export_onnx.endswith(\".onnx\"), \"Export model file should end with .onnx\"\n",
    "            output = torch.onnx._export(style_model, content_image, args.export_onnx).cpu()\n",
    "        else:\n",
    "            output = style_model(content_image).cpu()\n",
    "utils.save_image(args.output_image, output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-b887ff2690b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_onnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Read ONNX model and run it using Caffe2\n",
    "\"\"\"\n",
    "\n",
    "assert not args.export_onnx\n",
    "\n",
    "import onnx\n",
    "import onnx_caffe2.backend\n",
    "\n",
    "model = onnx.load(args.model)\n",
    "\n",
    "prepared_backend = onnx_caffe2.backend.prepare(model, device='CUDA' if args.cuda else 'CPU')\n",
    "inp = {model.graph.input[0].name: content_image.numpy()}\n",
    "c2_out = prepared_backend.run(inp)[0]\n",
    "\n",
    "return torch.from_numpy(c2_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
